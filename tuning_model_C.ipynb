{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f9b0dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import the library\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import reciprocal\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "import math\n",
    "\n",
    "np.random.seed(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db1ccef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7100 entries, 0 to 7099\n",
      "Data columns (total 21 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   case_id              7100 non-null   int64  \n",
      " 1   Experiment ID        7100 non-null   int64  \n",
      " 2   Date Time            7100 non-null   object \n",
      " 3   time                 7100 non-null   int64  \n",
      " 4   Gas Inje Cum SCTR    7100 non-null   float64\n",
      " 5   cum_co2              7100 non-null   float64\n",
      " 6   Water Prod Cum SCTR  7100 non-null   float64\n",
      " 7   cum_brine            7100 non-null   float64\n",
      " 8   i_inj                7100 non-null   int64  \n",
      " 9   j_inj                7100 non-null   int64  \n",
      " 10  i_pro                7100 non-null   int64  \n",
      " 11  j_pro                7100 non-null   int64  \n",
      " 12  k                    7100 non-null   int64  \n",
      " 13  ijk_inj              7100 non-null   object \n",
      " 14  ijk_pro              7100 non-null   object \n",
      " 15  distance             7100 non-null   float64\n",
      " 16  min_distance_inj     7100 non-null   float64\n",
      " 17  perm_avg_inj         7100 non-null   float64\n",
      " 18  perm_avg_pro         7100 non-null   float64\n",
      " 19  por_avg_inj          7100 non-null   float64\n",
      " 20  por_avg_pro          7100 non-null   float64\n",
      "dtypes: float64(10), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>Experiment ID</th>\n",
       "      <th>time</th>\n",
       "      <th>Gas Inje Cum SCTR</th>\n",
       "      <th>cum_co2</th>\n",
       "      <th>Water Prod Cum SCTR</th>\n",
       "      <th>cum_brine</th>\n",
       "      <th>i_inj</th>\n",
       "      <th>j_inj</th>\n",
       "      <th>i_pro</th>\n",
       "      <th>j_pro</th>\n",
       "      <th>k</th>\n",
       "      <th>distance</th>\n",
       "      <th>min_distance_inj</th>\n",
       "      <th>perm_avg_inj</th>\n",
       "      <th>perm_avg_pro</th>\n",
       "      <th>por_avg_inj</th>\n",
       "      <th>por_avg_pro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7.100000e+03</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.0</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.500000</td>\n",
       "      <td>372.220000</td>\n",
       "      <td>1095.492958</td>\n",
       "      <td>5.065765e+07</td>\n",
       "      <td>0.050658</td>\n",
       "      <td>117684.886631</td>\n",
       "      <td>0.117685</td>\n",
       "      <td>49.340000</td>\n",
       "      <td>88.060000</td>\n",
       "      <td>44.030000</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.885670</td>\n",
       "      <td>13.504821</td>\n",
       "      <td>63.280290</td>\n",
       "      <td>66.561501</td>\n",
       "      <td>0.156270</td>\n",
       "      <td>0.155713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28.868103</td>\n",
       "      <td>215.827413</td>\n",
       "      <td>623.816256</td>\n",
       "      <td>3.127186e+07</td>\n",
       "      <td>0.031272</td>\n",
       "      <td>103635.381933</td>\n",
       "      <td>0.103635</td>\n",
       "      <td>22.006649</td>\n",
       "      <td>32.304232</td>\n",
       "      <td>21.871362</td>\n",
       "      <td>34.052178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.367113</td>\n",
       "      <td>9.303333</td>\n",
       "      <td>20.139956</td>\n",
       "      <td>20.385542</td>\n",
       "      <td>0.012030</td>\n",
       "      <td>0.011350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000352e+06</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.099020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.737714</td>\n",
       "      <td>12.895617</td>\n",
       "      <td>0.128144</td>\n",
       "      <td>0.125934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.750000</td>\n",
       "      <td>176.500000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>2.445799e+07</td>\n",
       "      <td>0.024458</td>\n",
       "      <td>26708.427500</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>65.750000</td>\n",
       "      <td>26.750000</td>\n",
       "      <td>65.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.778197</td>\n",
       "      <td>5.719505</td>\n",
       "      <td>52.166635</td>\n",
       "      <td>50.576358</td>\n",
       "      <td>0.147317</td>\n",
       "      <td>0.147501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.500000</td>\n",
       "      <td>365.500000</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>4.346827e+07</td>\n",
       "      <td>0.043468</td>\n",
       "      <td>91441.910000</td>\n",
       "      <td>0.091442</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.273392</td>\n",
       "      <td>12.369317</td>\n",
       "      <td>63.250876</td>\n",
       "      <td>64.576557</td>\n",
       "      <td>0.155804</td>\n",
       "      <td>0.156415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>75.250000</td>\n",
       "      <td>586.000000</td>\n",
       "      <td>1643.000000</td>\n",
       "      <td>6.921627e+07</td>\n",
       "      <td>0.069216</td>\n",
       "      <td>178301.985000</td>\n",
       "      <td>0.178302</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>113.250000</td>\n",
       "      <td>61.250000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.094047</td>\n",
       "      <td>21.112237</td>\n",
       "      <td>76.902190</td>\n",
       "      <td>82.102771</td>\n",
       "      <td>0.165277</td>\n",
       "      <td>0.164880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>2161.000000</td>\n",
       "      <td>1.680000e+08</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>526227.380000</td>\n",
       "      <td>0.526227</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>159.062881</td>\n",
       "      <td>35.777088</td>\n",
       "      <td>99.849706</td>\n",
       "      <td>106.545882</td>\n",
       "      <td>0.180835</td>\n",
       "      <td>0.179235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           case_id  Experiment ID         time  Gas Inje Cum SCTR  \\\n",
       "count  7100.000000    7100.000000  7100.000000       7.100000e+03   \n",
       "mean     50.500000     372.220000  1095.492958       5.065765e+07   \n",
       "std      28.868103     215.827413   623.816256       3.127186e+07   \n",
       "min       1.000000      30.000000    31.000000       1.000352e+06   \n",
       "25%      25.750000     176.500000   547.000000       2.445799e+07   \n",
       "50%      50.500000     365.500000  1096.000000       4.346827e+07   \n",
       "75%      75.250000     586.000000  1643.000000       6.921627e+07   \n",
       "max     100.000000     716.000000  2161.000000       1.680000e+08   \n",
       "\n",
       "           cum_co2  Water Prod Cum SCTR    cum_brine        i_inj  \\\n",
       "count  7100.000000          7100.000000  7100.000000  7100.000000   \n",
       "mean      0.050658        117684.886631     0.117685    49.340000   \n",
       "std       0.031272        103635.381933     0.103635    22.006649   \n",
       "min       0.001000             0.000000     0.000000     1.000000   \n",
       "25%       0.024458         26708.427500     0.026708    34.000000   \n",
       "50%       0.043468         91441.910000     0.091442    49.500000   \n",
       "75%       0.069216        178301.985000     0.178302    68.000000   \n",
       "max       0.168000        526227.380000     0.526227    95.000000   \n",
       "\n",
       "             j_inj        i_pro        j_pro       k     distance  \\\n",
       "count  7100.000000  7100.000000  7100.000000  7100.0  7100.000000   \n",
       "mean     88.060000    44.030000    91.250000     1.0    52.885670   \n",
       "std      32.304232    21.871362    34.052178     0.0    29.367113   \n",
       "min       1.000000     1.000000     1.000000     1.0     5.099020   \n",
       "25%      65.750000    26.750000    65.750000     1.0    29.778197   \n",
       "50%      92.500000    44.000000    97.500000     1.0    50.273392   \n",
       "75%     113.250000    61.250000   117.000000     1.0    69.094047   \n",
       "max     152.000000    95.000000   152.000000     1.0   159.062881   \n",
       "\n",
       "       min_distance_inj  perm_avg_inj  perm_avg_pro  por_avg_inj  por_avg_pro  \n",
       "count       7100.000000   7100.000000   7100.000000  7100.000000  7100.000000  \n",
       "mean          13.504821     63.280290     66.561501     0.156270     0.155713  \n",
       "std            9.303333     20.139956     20.385542     0.012030     0.011350  \n",
       "min            0.000000     13.737714     12.895617     0.128144     0.125934  \n",
       "25%            5.719505     52.166635     50.576358     0.147317     0.147501  \n",
       "50%           12.369317     63.250876     64.576557     0.155804     0.156415  \n",
       "75%           21.112237     76.902190     82.102771     0.165277     0.164880  \n",
       "max           35.777088     99.849706    106.545882     0.180835     0.179235  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read and peek into the data\n",
    "df=pd.read_csv('thesis_sample_100_shuffled_w_respect_to_id_Model_C.csv')\n",
    "print(df.info())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54750e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                0            1            2            3            4   \\\n",
       " count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000   \n",
       " mean     49.340000    88.060000    44.030000    91.250000  1095.492958   \n",
       " std      22.006649    32.304232    21.871362    34.052178   623.816256   \n",
       " min       1.000000     1.000000     1.000000     1.000000    31.000000   \n",
       " 25%      34.000000    65.750000    26.750000    65.750000   547.000000   \n",
       " 50%      49.500000    92.500000    44.000000    97.500000  1096.000000   \n",
       " 75%      68.000000   113.250000    61.250000   117.000000  1643.000000   \n",
       " max      95.000000   152.000000    95.000000   152.000000  2161.000000   \n",
       " \n",
       "                 5            6            7            8            9   \\\n",
       " count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000   \n",
       " mean     63.280290    66.561501     0.156270     0.155713    13.504821   \n",
       " std      20.139956    20.385542     0.012030     0.011350     9.303333   \n",
       " min      13.737714    12.895617     0.128144     0.125934     0.000000   \n",
       " 25%      52.166635    50.576358     0.147317     0.147501     5.719505   \n",
       " 50%      63.250876    64.576557     0.155804     0.156415    12.369317   \n",
       " 75%      76.902190    82.102771     0.165277     0.164880    21.112237   \n",
       " max      99.849706   106.545882     0.180835     0.179235    35.777088   \n",
       " \n",
       "                 10  \n",
       " count  7100.000000  \n",
       " mean     52.885670  \n",
       " std      29.367113  \n",
       " min       5.099020  \n",
       " 25%      29.778197  \n",
       " 50%      50.273392  \n",
       " 75%      69.094047  \n",
       " max     159.062881  ,\n",
       "                  0\n",
       " count  7100.000000\n",
       " mean      0.050658\n",
       " std       0.031272\n",
       " min       0.001000\n",
       " 25%       0.024458\n",
       " 50%       0.043468\n",
       " 75%       0.069216\n",
       " max       0.168000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this converts the panda dataframe into ndarray using function to_numpy\n",
    "x=df[['i_inj','j_inj','i_pro','j_pro','time',\n",
    "      'perm_avg_inj','perm_avg_pro','por_avg_inj','por_avg_pro',\n",
    "      'min_distance_inj','distance']].to_numpy() \n",
    "\n",
    "#same as above converted to nd array using to_numpy function\n",
    "y=df[['cum_co2']].to_numpy() \n",
    "\n",
    "pd.DataFrame(x).describe(),pd.DataFrame(y).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6270cd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.514255</td>\n",
       "      <td>0.576556</td>\n",
       "      <td>0.457766</td>\n",
       "      <td>0.597682</td>\n",
       "      <td>0.499762</td>\n",
       "      <td>0.575327</td>\n",
       "      <td>0.573046</td>\n",
       "      <td>0.533788</td>\n",
       "      <td>0.558684</td>\n",
       "      <td>0.377471</td>\n",
       "      <td>0.310376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.234113</td>\n",
       "      <td>0.213935</td>\n",
       "      <td>0.232674</td>\n",
       "      <td>0.225511</td>\n",
       "      <td>0.292871</td>\n",
       "      <td>0.233881</td>\n",
       "      <td>0.217677</td>\n",
       "      <td>0.228314</td>\n",
       "      <td>0.212942</td>\n",
       "      <td>0.260036</td>\n",
       "      <td>0.190740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.428808</td>\n",
       "      <td>0.273936</td>\n",
       "      <td>0.428808</td>\n",
       "      <td>0.242254</td>\n",
       "      <td>0.446267</td>\n",
       "      <td>0.402356</td>\n",
       "      <td>0.363873</td>\n",
       "      <td>0.404624</td>\n",
       "      <td>0.159865</td>\n",
       "      <td>0.160292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.515957</td>\n",
       "      <td>0.605960</td>\n",
       "      <td>0.457447</td>\n",
       "      <td>0.639073</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.574986</td>\n",
       "      <td>0.551850</td>\n",
       "      <td>0.524946</td>\n",
       "      <td>0.571864</td>\n",
       "      <td>0.345733</td>\n",
       "      <td>0.293409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.712766</td>\n",
       "      <td>0.743377</td>\n",
       "      <td>0.640957</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>0.756808</td>\n",
       "      <td>0.733515</td>\n",
       "      <td>0.738996</td>\n",
       "      <td>0.704719</td>\n",
       "      <td>0.730678</td>\n",
       "      <td>0.590105</td>\n",
       "      <td>0.415650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000   \n",
       "mean      0.514255     0.576556     0.457766     0.597682     0.499762   \n",
       "std       0.234113     0.213935     0.232674     0.225511     0.292871   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.351064     0.428808     0.273936     0.428808     0.242254   \n",
       "50%       0.515957     0.605960     0.457447     0.639073     0.500000   \n",
       "75%       0.712766     0.743377     0.640957     0.768212     0.756808   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000   \n",
       "mean      0.575327     0.573046     0.533788     0.558684     0.377471   \n",
       "std       0.233881     0.217677     0.228314     0.212942     0.260036   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.446267     0.402356     0.363873     0.404624     0.159865   \n",
       "50%       0.574986     0.551850     0.524946     0.571864     0.345733   \n",
       "75%       0.733515     0.738996     0.704719     0.730678     0.590105   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                10  \n",
       "count  7100.000000  \n",
       "mean      0.310376  \n",
       "std       0.190740  \n",
       "min       0.000000  \n",
       "25%       0.160292  \n",
       "50%       0.293409  \n",
       "75%       0.415650  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scale the training data\n",
    "scaler=MinMaxScaler()\n",
    "train_data=scaler.fit_transform(x) # it is important to fit the scaler into training set only\n",
    "pd.DataFrame(train_data).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558f4316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.050658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.031272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.024458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.043468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.069216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  7100.000000\n",
       "mean      0.050658\n",
       "std       0.031272\n",
       "min       0.001000\n",
       "25%       0.024458\n",
       "50%       0.043468\n",
       "75%       0.069216\n",
       "max       0.168000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets=y\n",
    "\n",
    "pd.DataFrame(train_targets).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0191f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(params):\n",
    "    #convert the best solution into a layer sizes, integer values from floats\n",
    "    # transform the layer sizes from float (possibly negative) values into hiddenLayerSizes tuple:\n",
    "    if round(params[1]) <= 9:\n",
    "        hiddenLayerSizes = round(params[0]),\n",
    "    elif round(params[2]) <= 9:\n",
    "        hiddenLayerSizes = (round(params[0]), round(params[1]))\n",
    "    elif round(params[3]) <= 9:\n",
    "        hiddenLayerSizes = (round(params[0]), round(params[1]), round(params[2]))\n",
    "    elif round(params[4]) <= 9:\n",
    "        hiddenLayerSizes = (round(params[0]), round(params[1]), round(params[2]), round(params[3]))\n",
    "    else :\n",
    "        hiddenLayerSizes = (round(params[0]), round(params[1]), round(params[2]), round(params[3]), \n",
    "                            round(params[4]))\n",
    "\n",
    "    return hiddenLayerSizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b204eb10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(individuals):\n",
    "    # transform the layer sizes from float (possibly negative) values into hiddenLayerSizes tuple:\n",
    "    hiddenLayerSizes=converter(individuals)\n",
    "    #print (hiddenLayerSizes,len(hiddenLayerSizes))\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayerSizes[0],input_dim=train_data.shape[1],activation='relu'))\n",
    "    \n",
    "    \n",
    "    if len(hiddenLayerSizes)>1:\n",
    "        for i in range(len(hiddenLayerSizes)-1):\n",
    "            model.add(keras.layers.Dense(hiddenLayerSizes[i+1],activation='relu')) #hidden layer generator loop\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "        \n",
    "    learning_rate=1e-3\n",
    "    #print (learning_rate)\n",
    "\n",
    "    \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.compile(loss='mse', metrics=['mae'],\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate))  \n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c544ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_round_generator(params):\n",
    "    rounded_ind = (converter(params))\n",
    "    return rounded_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7b9275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_validation(individual):\n",
    "\n",
    "    num_epochs = 20\n",
    "    k = 4\n",
    "    num_val_samples = len(train_data) // k\n",
    "    #print('num_val_samples:',num_val_samples)\n",
    "    all_scores = [] \n",
    "    \n",
    "    batch_size=32\n",
    "    #rint ('batch_size',batch_size)\n",
    "    \n",
    "    for i in range(k):\n",
    "        #print(f\"Processing fold #{i}\")    \n",
    "        val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]     \n",
    "        val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]    \n",
    "        partial_train_data = np.concatenate([train_data[:i * num_val_samples],         \n",
    "                                                 train_data[(i + 1) * num_val_samples:]],axis=0)    \n",
    "        partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],         \n",
    "                                                    train_targets[(i + 1) * num_val_samples:]],axis=0)   \n",
    "        \n",
    "        model = build_model(individual)\n",
    "    \n",
    "        history = model.fit(partial_train_data, partial_train_targets,validation_data=(val_data, val_targets),\n",
    "                            epochs=num_epochs, batch_size=batch_size, verbose=0, callbacks=None)  \n",
    "        \n",
    "        val_mae_loss, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "        all_scores.append(val_mae)\n",
    "        \n",
    "    f=np.mean(all_scores)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24c99ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process began\n",
      "gen\tnevals\tmin       \tavg      \n",
      "0  \t30    \t0.00939975\t0.0141386\n",
      "Processing gen #1\n",
      "1  \t24    \t0.00906153\t0.012829 \n",
      "# of layers: 5 best_individual (99, 69, 25, 11, 38) Fitness (0.009399753296747804,)\n",
      "Processing gen #2\n",
      "2  \t24    \t0.00906153\t0.0117216\n",
      "# of layers: 5 best_individual (99, 69, 28, 10, 38) Fitness (0.009061526041477919,)\n",
      "Processing gen #3\n",
      "3  \t28    \t0.00894903\t0.0110867\n",
      "# of layers: 5 best_individual (99, 69, 28, 10, 38) Fitness (0.009061526041477919,)\n",
      "Processing gen #4\n",
      "4  \t25    \t0.00848385\t0.0105854\n",
      "# of layers: 5 best_individual (99, 69, 28, 10, 82) Fitness (0.00894902809523046,)\n",
      "Processing gen #5\n",
      "5  \t24    \t0.00848385\t0.0106952\n",
      "# of layers: 5 best_individual (99, 69, 55, 61, 68) Fitness (0.008483851794153452,)\n",
      "Processing gen #6\n",
      "6  \t24    \t0.00848385\t0.0104345\n",
      "# of layers: 5 best_individual (99, 69, 55, 61, 68) Fitness (0.008483851794153452,)\n",
      "Processing gen #7\n",
      "7  \t24    \t0.00848385\t0.00983275\n",
      "# of layers: 5 best_individual (99, 69, 55, 61, 68) Fitness (0.008483851794153452,)\n",
      "Processing gen #8\n",
      "8  \t26    \t0.00848385\t0.00978681\n",
      "# of layers: 5 best_individual (99, 69, 55, 61, 68) Fitness (0.008483851794153452,)\n",
      "Processing gen #9\n",
      "9  \t26    \t0.00848385\t0.00993629\n",
      "# of layers: 5 best_individual (99, 69, 55, 61, 68) Fitness (0.008483851794153452,)\n",
      "Processing gen #10\n",
      "10 \t25    \t0.00848385\t0.00995984\n",
      "# of layers: 5 best_individual (99, 69, 55, 61, 68) Fitness (0.008483851794153452,)\n",
      "Processing gen #11\n",
      "11 \t28    \t0.00848385\t0.0102581 \n",
      "# of layers: 5 best_individual (99, 69, 55, 61, 68) Fitness (0.008483851794153452,)\n",
      "Processing gen #12\n",
      "12 \t27    \t0.00844912\t0.00987989\n",
      "# of layers: 5 best_individual (99, 69, 55, 61, 68) Fitness (0.008483851794153452,)\n",
      "Processing gen #13\n",
      "13 \t27    \t0.00844912\t0.0103161 \n",
      "# of layers: 5 best_individual (99, 69, 28, 19, 64) Fitness (0.008449117420241237,)\n",
      "Processing gen #14\n",
      "14 \t28    \t0.00844912\t0.00995092\n",
      "# of layers: 5 best_individual (99, 69, 28, 19, 64) Fitness (0.008449117420241237,)\n",
      "Processing gen #15\n",
      "15 \t23    \t0.00843412\t0.00993332\n",
      "# of layers: 5 best_individual (99, 69, 28, 19, 64) Fitness (0.008449117420241237,)\n",
      "Processing gen #16\n",
      "16 \t25    \t0.00843412\t0.0099281 \n",
      "# of layers: 5 best_individual (96, 69, 24, 74, 37) Fitness (0.008434123359620571,)\n",
      "Processing gen #17\n",
      "17 \t27    \t0.00843412\t0.0101454 \n",
      "# of layers: 5 best_individual (96, 69, 24, 74, 37) Fitness (0.008434123359620571,)\n",
      "Processing gen #18\n",
      "18 \t28    \t0.00843412\t0.0100325 \n",
      "# of layers: 5 best_individual (96, 69, 24, 74, 37) Fitness (0.008434123359620571,)\n",
      "Processing gen #19\n",
      "19 \t24    \t0.00841613\t0.00974924\n",
      "# of layers: 5 best_individual (96, 69, 24, 74, 37) Fitness (0.008434123359620571,)\n",
      "Processing gen #20\n",
      "20 \t27    \t0.00841613\t0.00984868\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #21\n",
      "21 \t26    \t0.00841613\t0.00976984\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #22\n",
      "22 \t25    \t0.00841613\t0.00980365\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #23\n",
      "23 \t27    \t0.00841613\t0.00959841\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #24\n",
      "24 \t27    \t0.00841613\t0.00984054\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #25\n",
      "25 \t24    \t0.00841613\t0.00989769\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #26\n",
      "26 \t28    \t0.00841613\t0.00980543\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #27\n",
      "27 \t27    \t0.00841613\t0.00974547\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #28\n",
      "28 \t28    \t0.00841613\t0.00996631\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #29\n",
      "29 \t26    \t0.00841613\t0.0100017 \n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #30\n",
      "30 \t28    \t0.00841613\t0.00971597\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #31\n",
      "31 \t22    \t0.00841613\t0.00973649\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #32\n",
      "32 \t28    \t0.00841613\t0.009679  \n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #33\n",
      "33 \t27    \t0.00841613\t0.00955004\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #34\n",
      "34 \t28    \t0.00841613\t0.00972591\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #35\n",
      "35 \t22    \t0.00841613\t0.00951562\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #36\n",
      "36 \t28    \t0.00841613\t0.00982671\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #37\n",
      "37 \t27    \t0.00841613\t0.00965356\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #38\n",
      "38 \t26    \t0.00841613\t0.00956374\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #39\n",
      "39 \t28    \t0.00841613\t0.00959568\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #40\n",
      "40 \t26    \t0.00841613\t0.00964251\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #41\n",
      "41 \t28    \t0.00841613\t0.00965636\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #42\n",
      "42 \t24    \t0.00841613\t0.00973758\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #43\n",
      "43 \t28    \t0.00841613\t0.00962245\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #44\n",
      "44 \t25    \t0.00841613\t0.00966431\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #45\n",
      "45 \t25    \t0.00841613\t0.00978346\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #46\n",
      "46 \t25    \t0.00841613\t0.00994127\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #47\n",
      "47 \t25    \t0.00841613\t0.00983969\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #48\n",
      "48 \t27    \t0.00841613\t0.00969374\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #49\n",
      "49 \t26    \t0.00841613\t0.00965701\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "Processing gen #50\n",
      "50 \t28    \t0.00841613\t0.00982416\n",
      "# of layers: 5 best_individual (99, 69, 27, 60, 71) Fitness (0.008416132652200758,)\n",
      "- GA Best solution before converting is:  [98.8657916734722, 69.34960966389019, 27.054654551053474, 59.99230204929498, 71.2113426383838] ,loss is: 0.008416132652200758\n",
      "- GA Best solution after converting - Layers and unit numbers:  (99, 69, 27, 60, 71) , Loss =  0.008416132652200758\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    print('The process began')\n",
    "    \n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "    \n",
    "    #iterator is required to iterate in elements of tuple logbook.select(\"max\")\n",
    "    iterator=0\n",
    "    #create a numpy array for later use as container and write best individuals to csv file\n",
    "    all_in_one=np.zeros([1,7])\n",
    "    \n",
    "    \n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "        \n",
    "        print(f\"Processing gen #{gen}\") \n",
    "            \n",
    "        \n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "            \n",
    "        \n",
    "        #loop is for printing out the best ind and fitness values of each generation\n",
    "        for ind in population:\n",
    "           \n",
    "            if ind.fitness.values==logbook.select(\"min\")[iterator]: \n",
    "                print('# of layers:',len(ind_round_generator(ind)),'best_individual',ind_round_generator(ind),\n",
    "                      'Fitness',ind.fitness.values)\n",
    "                tuple1=tuple(ind) #convert individual into tuple\n",
    "                tuple2=tuple(ind.fitness.values) #convert individual fitness into tuple\n",
    "                tuple3=(gen-1,)+tuple1+tuple2\n",
    "                #combine generation, individual, fitnesses into one tuple\n",
    "                to_array=np.asarray(tuple3)  #convert combined tuple into an array\n",
    "                reshaped=np.reshape(to_array,(1,7)) #reshape array\n",
    "                all_in_one=np.append(all_in_one,reshaped,axis=0) #add new values in a new row using axis=0 option\n",
    "                break #do not output all ind with highest fitness value\n",
    "        iterator+=1\n",
    "    \n",
    "    return population, logbook\n",
    "\n",
    "\n",
    "# boundaries for layer size parameters: \n",
    "             # [0,     1     2,     3     4     5      \n",
    "BOUNDS_LOW =  [   10,    0,   -20,  -50,  -100 ]\n",
    "              \n",
    "BOUNDS_HIGH = [   100,  100,  100,   100,  100]\n",
    "\n",
    "\n",
    "NUM_OF_PARAMS = len(BOUNDS_HIGH)\n",
    "\n",
    "# Genetic Algorithm constants:\n",
    "POPULATION_SIZE = 30\n",
    "P_CROSSOVER = 0.9  # probability for crossover\n",
    "P_MUTATION = 0.3   # probability for mutating an individual\n",
    "MAX_GENERATIONS = 50\n",
    "HALL_OF_FAME_SIZE = 2\n",
    "CROWDING_FACTOR = 10.0  # crowding factor for crossover and mutation\n",
    "\n",
    "# set the random seed:\n",
    "RANDOM_SEED = 44\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# define a single objective, maximizing fitness strategy:\n",
    "creator.create(\"FitnessMax_tuning_model_c\", base.Fitness, weights=(-1.0,))\n",
    "\n",
    "# create the Individual class based on list:\n",
    "creator.create(\"Individual_tuning_model_c\", list, \n",
    "               fitness=creator.FitnessMax_tuning_model_c)\n",
    "\n",
    "# define the layer_size_attributes individually:\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    # \"layer_size_attribute_0\", \"layer_size_attribute_1\", ...\n",
    "    toolbox.register(\"layer_size_attribute_\" + str(i),\n",
    "                     random.uniform,\n",
    "                     BOUNDS_LOW[i],\n",
    "                     BOUNDS_HIGH[i])\n",
    "\n",
    "# create a tuple containing an layer_size_attribute generator for each hidden layer:\n",
    "layer_size_attributes = ()\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    layer_size_attributes = layer_size_attributes + \\\n",
    "                            (toolbox.__getattribute__(\"layer_size_attribute_\" + str(i)),)\n",
    "\n",
    "# create the individual operator to fill up an Individual instance:\n",
    "toolbox.register(\"individualCreator\",\n",
    "                 tools.initCycle,\n",
    "                 creator.Individual_tuning_model_c,\n",
    "                 layer_size_attributes,\n",
    "                 n=1)\n",
    "\n",
    "# create the population operator to generate a list of individuals:\n",
    "toolbox.register(\"populationCreator\",\n",
    "                 tools.initRepeat,\n",
    "                 list,\n",
    "                 toolbox.individualCreator)\n",
    "\n",
    "\n",
    "# fitness calculation\n",
    "def accuracy(individual):\n",
    "    fitness=k_fold_validation(individual)\n",
    "    #print (type(fitness))\n",
    "    return fitness,\n",
    "\n",
    "\n",
    "toolbox.register(\"evaluate\", accuracy)\n",
    "\n",
    "# genetic operators:mutFlipBit\n",
    "\n",
    "# genetic operators:\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=2)\n",
    "\n",
    "toolbox.register(\"mate\",\n",
    "                 tools.cxSimulatedBinaryBounded,\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR)\n",
    "\n",
    "toolbox.register(\"mutate\",\n",
    "                 tools.mutPolynomialBounded,\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR,\n",
    "                 indpb=1.0/NUM_OF_PARAMS)\n",
    "\n",
    "\n",
    "# Genetic Algorithm flow:\n",
    "def main():\n",
    "\n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.populationCreator(n=POPULATION_SIZE)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # perform the Genetic Algorithm flow with hof feature added:\n",
    "    population, logbook = eaSimpleWithElitism(population,\n",
    "                                                      toolbox,\n",
    "                                                      cxpb=P_CROSSOVER,\n",
    "                                                      mutpb=P_MUTATION,\n",
    "                                                      ngen=MAX_GENERATIONS,\n",
    "                                                      stats=stats,\n",
    "                                                      halloffame=hof,\n",
    "                                                      verbose=True)\n",
    "    #print all of the best solutions found:    \n",
    "       \n",
    "    print(\"- GA Best solution before converting is: \",hof.items[0],\",loss is:\", hof.items[0].fitness.values[0])\n",
    "    print(\"- GA Best solution after converting - Layers and unit numbers: \",converter(hof.items[0]),\n",
    "          \", Loss = \", hof.items[0].fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues = logbook.select(\"min\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='red')\n",
    "    plt.plot(meanFitnessValues, color='green')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Min / Average Fitness')\n",
    "    plt.title('Min and Average fitness over Generations')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#execute the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db00e832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
