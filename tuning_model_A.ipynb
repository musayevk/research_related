{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f9b0dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import the library\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import reciprocal\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "import math\n",
    "\n",
    "np.random.seed(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db1ccef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7100 entries, 0 to 7099\n",
      "Data columns (total 65 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   case_id              7100 non-null   int64  \n",
      " 1   Experiment ID        7100 non-null   int64  \n",
      " 2   Date Time            7100 non-null   object \n",
      " 3   time                 7100 non-null   int64  \n",
      " 4   Gas Inje Cum SCTR    7100 non-null   float64\n",
      " 5   cum_co2              7100 non-null   float64\n",
      " 6   Water Prod Cum SCTR  7100 non-null   float64\n",
      " 7   cum_brine            7100 non-null   float64\n",
      " 8   i_inj                7100 non-null   int64  \n",
      " 9   j_inj                7100 non-null   int64  \n",
      " 10  i_pro                7100 non-null   int64  \n",
      " 11  j_pro                7100 non-null   int64  \n",
      " 12  k                    7100 non-null   int64  \n",
      " 13  ijk_inj              7100 non-null   object \n",
      " 14  ijk_pro              7100 non-null   object \n",
      " 15  distance             7100 non-null   float64\n",
      " 16  min_distance_inj     7100 non-null   float64\n",
      " 17  k1_inj               7100 non-null   float64\n",
      " 18  k2_inj               7100 non-null   float64\n",
      " 19  k3_inj               7100 non-null   float64\n",
      " 20  k4_inj               7100 non-null   float64\n",
      " 21  k5_inj               7100 non-null   float64\n",
      " 22  k6_inj               7100 non-null   float64\n",
      " 23  k7_inj               7100 non-null   float64\n",
      " 24  k8_inj               7100 non-null   float64\n",
      " 25  k9_inj               7100 non-null   float64\n",
      " 26  k10_inj              7100 non-null   float64\n",
      " 27  k11_inj              7100 non-null   float64\n",
      " 28  k12_inj              7100 non-null   float64\n",
      " 29  k1_pro               7100 non-null   float64\n",
      " 30  k2_pro               7100 non-null   float64\n",
      " 31  k3_pro               7100 non-null   float64\n",
      " 32  k4_pro               7100 non-null   float64\n",
      " 33  k5_pro               7100 non-null   float64\n",
      " 34  k6_pro               7100 non-null   float64\n",
      " 35  k7_pro               7100 non-null   float64\n",
      " 36  k8_pro               7100 non-null   float64\n",
      " 37  k9_pro               7100 non-null   float64\n",
      " 38  k10_pro              7100 non-null   float64\n",
      " 39  k11_pro              7100 non-null   float64\n",
      " 40  k12_pro              7100 non-null   float64\n",
      " 41  por1_inj             7100 non-null   float64\n",
      " 42  por2_inj             7100 non-null   float64\n",
      " 43  por3_inj             7100 non-null   float64\n",
      " 44  por4_inj             7100 non-null   float64\n",
      " 45  por5_inj             7100 non-null   float64\n",
      " 46  por6_inj             7100 non-null   float64\n",
      " 47  por7_inj             7100 non-null   float64\n",
      " 48  por8_inj             7100 non-null   float64\n",
      " 49  por9_inj             7100 non-null   float64\n",
      " 50  por10_inj            7100 non-null   float64\n",
      " 51  por11_inj            7100 non-null   float64\n",
      " 52  por12_inj            7100 non-null   float64\n",
      " 53  por1_pro             7100 non-null   float64\n",
      " 54  por2_pro             7100 non-null   float64\n",
      " 55  por3_pro             7100 non-null   float64\n",
      " 56  por4_pro             7100 non-null   float64\n",
      " 57  por5_pro             7100 non-null   float64\n",
      " 58  por6_pro             7100 non-null   float64\n",
      " 59  por7_pro             7100 non-null   float64\n",
      " 60  por8_pro             7100 non-null   float64\n",
      " 61  por9_pro             7100 non-null   float64\n",
      " 62  por10_pro            7100 non-null   float64\n",
      " 63  por11_pro            7100 non-null   float64\n",
      " 64  por12_pro            7100 non-null   float64\n",
      "dtypes: float64(54), int64(8), object(3)\n",
      "memory usage: 3.5+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>Experiment ID</th>\n",
       "      <th>time</th>\n",
       "      <th>Gas Inje Cum SCTR</th>\n",
       "      <th>cum_co2</th>\n",
       "      <th>Water Prod Cum SCTR</th>\n",
       "      <th>cum_brine</th>\n",
       "      <th>i_inj</th>\n",
       "      <th>j_inj</th>\n",
       "      <th>i_pro</th>\n",
       "      <th>...</th>\n",
       "      <th>por3_pro</th>\n",
       "      <th>por4_pro</th>\n",
       "      <th>por5_pro</th>\n",
       "      <th>por6_pro</th>\n",
       "      <th>por7_pro</th>\n",
       "      <th>por8_pro</th>\n",
       "      <th>por9_pro</th>\n",
       "      <th>por10_pro</th>\n",
       "      <th>por11_pro</th>\n",
       "      <th>por12_pro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7.100000e+03</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.500000</td>\n",
       "      <td>372.220000</td>\n",
       "      <td>1095.492958</td>\n",
       "      <td>5.065765e+07</td>\n",
       "      <td>0.050658</td>\n",
       "      <td>117684.886631</td>\n",
       "      <td>0.117685</td>\n",
       "      <td>49.340000</td>\n",
       "      <td>88.060000</td>\n",
       "      <td>44.030000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322692</td>\n",
       "      <td>0.302917</td>\n",
       "      <td>0.251244</td>\n",
       "      <td>0.042201</td>\n",
       "      <td>0.151398</td>\n",
       "      <td>0.110634</td>\n",
       "      <td>0.043510</td>\n",
       "      <td>0.101082</td>\n",
       "      <td>0.094218</td>\n",
       "      <td>0.114990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28.868103</td>\n",
       "      <td>215.827413</td>\n",
       "      <td>623.816256</td>\n",
       "      <td>3.127186e+07</td>\n",
       "      <td>0.031272</td>\n",
       "      <td>103635.381933</td>\n",
       "      <td>0.103635</td>\n",
       "      <td>22.006649</td>\n",
       "      <td>32.304232</td>\n",
       "      <td>21.871362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022489</td>\n",
       "      <td>0.032085</td>\n",
       "      <td>0.032095</td>\n",
       "      <td>0.020304</td>\n",
       "      <td>0.047372</td>\n",
       "      <td>0.056237</td>\n",
       "      <td>0.019472</td>\n",
       "      <td>0.054633</td>\n",
       "      <td>0.058128</td>\n",
       "      <td>0.043888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000352e+06</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232294</td>\n",
       "      <td>0.198870</td>\n",
       "      <td>0.168496</td>\n",
       "      <td>0.011496</td>\n",
       "      <td>0.028061</td>\n",
       "      <td>0.011101</td>\n",
       "      <td>0.010377</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.750000</td>\n",
       "      <td>176.500000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>2.445799e+07</td>\n",
       "      <td>0.024458</td>\n",
       "      <td>26708.427500</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>65.750000</td>\n",
       "      <td>26.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311636</td>\n",
       "      <td>0.278293</td>\n",
       "      <td>0.232332</td>\n",
       "      <td>0.023597</td>\n",
       "      <td>0.122165</td>\n",
       "      <td>0.066971</td>\n",
       "      <td>0.026541</td>\n",
       "      <td>0.058052</td>\n",
       "      <td>0.033142</td>\n",
       "      <td>0.084602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.500000</td>\n",
       "      <td>365.500000</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>4.346827e+07</td>\n",
       "      <td>0.043468</td>\n",
       "      <td>91441.910000</td>\n",
       "      <td>0.091442</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334107</td>\n",
       "      <td>0.311130</td>\n",
       "      <td>0.259593</td>\n",
       "      <td>0.041357</td>\n",
       "      <td>0.158317</td>\n",
       "      <td>0.118826</td>\n",
       "      <td>0.040434</td>\n",
       "      <td>0.107728</td>\n",
       "      <td>0.102730</td>\n",
       "      <td>0.126339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>75.250000</td>\n",
       "      <td>586.000000</td>\n",
       "      <td>1643.000000</td>\n",
       "      <td>6.921627e+07</td>\n",
       "      <td>0.069216</td>\n",
       "      <td>178301.985000</td>\n",
       "      <td>0.178302</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>113.250000</td>\n",
       "      <td>61.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.329602</td>\n",
       "      <td>0.276931</td>\n",
       "      <td>0.058516</td>\n",
       "      <td>0.186858</td>\n",
       "      <td>0.156496</td>\n",
       "      <td>0.060423</td>\n",
       "      <td>0.145863</td>\n",
       "      <td>0.152218</td>\n",
       "      <td>0.153029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>2161.000000</td>\n",
       "      <td>1.680000e+08</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>526227.380000</td>\n",
       "      <td>0.526227</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.290151</td>\n",
       "      <td>0.079067</td>\n",
       "      <td>0.218336</td>\n",
       "      <td>0.196004</td>\n",
       "      <td>0.079772</td>\n",
       "      <td>0.182511</td>\n",
       "      <td>0.174827</td>\n",
       "      <td>0.168289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           case_id  Experiment ID         time  Gas Inje Cum SCTR  \\\n",
       "count  7100.000000    7100.000000  7100.000000       7.100000e+03   \n",
       "mean     50.500000     372.220000  1095.492958       5.065765e+07   \n",
       "std      28.868103     215.827413   623.816256       3.127186e+07   \n",
       "min       1.000000      30.000000    31.000000       1.000352e+06   \n",
       "25%      25.750000     176.500000   547.000000       2.445799e+07   \n",
       "50%      50.500000     365.500000  1096.000000       4.346827e+07   \n",
       "75%      75.250000     586.000000  1643.000000       6.921627e+07   \n",
       "max     100.000000     716.000000  2161.000000       1.680000e+08   \n",
       "\n",
       "           cum_co2  Water Prod Cum SCTR    cum_brine        i_inj  \\\n",
       "count  7100.000000          7100.000000  7100.000000  7100.000000   \n",
       "mean      0.050658        117684.886631     0.117685    49.340000   \n",
       "std       0.031272        103635.381933     0.103635    22.006649   \n",
       "min       0.001000             0.000000     0.000000     1.000000   \n",
       "25%       0.024458         26708.427500     0.026708    34.000000   \n",
       "50%       0.043468         91441.910000     0.091442    49.500000   \n",
       "75%       0.069216        178301.985000     0.178302    68.000000   \n",
       "max       0.168000        526227.380000     0.526227    95.000000   \n",
       "\n",
       "             j_inj        i_pro  ...     por3_pro     por4_pro     por5_pro  \\\n",
       "count  7100.000000  7100.000000  ...  7100.000000  7100.000000  7100.000000   \n",
       "mean     88.060000    44.030000  ...     0.322692     0.302917     0.251244   \n",
       "std      32.304232    21.871362  ...     0.022489     0.032085     0.032095   \n",
       "min       1.000000     1.000000  ...     0.232294     0.198870     0.168496   \n",
       "25%      65.750000    26.750000  ...     0.311636     0.278293     0.232332   \n",
       "50%      92.500000    44.000000  ...     0.334107     0.311130     0.259593   \n",
       "75%     113.250000    61.250000  ...     0.339900     0.329602     0.276931   \n",
       "max     152.000000    95.000000  ...     0.339900     0.339900     0.290151   \n",
       "\n",
       "          por6_pro     por7_pro     por8_pro     por9_pro    por10_pro  \\\n",
       "count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000   \n",
       "mean      0.042201     0.151398     0.110634     0.043510     0.101082   \n",
       "std       0.020304     0.047372     0.056237     0.019472     0.054633   \n",
       "min       0.011496     0.028061     0.011101     0.010377     0.011100   \n",
       "25%       0.023597     0.122165     0.066971     0.026541     0.058052   \n",
       "50%       0.041357     0.158317     0.118826     0.040434     0.107728   \n",
       "75%       0.058516     0.186858     0.156496     0.060423     0.145863   \n",
       "max       0.079067     0.218336     0.196004     0.079772     0.182511   \n",
       "\n",
       "         por11_pro    por12_pro  \n",
       "count  7100.000000  7100.000000  \n",
       "mean      0.094218     0.114990  \n",
       "std       0.058128     0.043888  \n",
       "min       0.011100     0.011100  \n",
       "25%       0.033142     0.084602  \n",
       "50%       0.102730     0.126339  \n",
       "75%       0.152218     0.153029  \n",
       "max       0.174827     0.168289  \n",
       "\n",
       "[8 rows x 62 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read and peek into the data\n",
    "df=pd.read_csv('thesis_sample_100_shuffled_w_respect_to_id_Model_A.csv')\n",
    "print(df.info())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54750e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                0            1            2            3            4   \\\n",
       " count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000   \n",
       " mean     49.340000    88.060000    44.030000    91.250000  1095.492958   \n",
       " std      22.006649    32.304232    21.871362    34.052178   623.816256   \n",
       " min       1.000000     1.000000     1.000000     1.000000    31.000000   \n",
       " 25%      34.000000    65.750000    26.750000    65.750000   547.000000   \n",
       " 50%      49.500000    92.500000    44.000000    97.500000  1096.000000   \n",
       " 75%      68.000000   113.250000    61.250000   117.000000  1643.000000   \n",
       " max      95.000000   152.000000    95.000000   152.000000  2161.000000   \n",
       " \n",
       "                 5            6            7            8            9   ...  \\\n",
       " count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000  ...   \n",
       " mean      0.010244   220.122031   277.623716   193.876243    59.050712  ...   \n",
       " std       0.009538   133.162587   119.037944   110.715096    36.263130  ...   \n",
       " min       0.001723    14.897340    28.709990    17.080750     1.611398  ...   \n",
       " 25%       0.001723    92.075212   185.197175    94.877527    23.842155  ...   \n",
       " 50%       0.005381   219.216400   324.585150   180.102950    61.755575  ...   \n",
       " 75%       0.020935   371.452500   386.917200   279.117825    94.958538  ...   \n",
       " max       0.025172   386.917200   386.917200   386.917200   130.040900  ...   \n",
       " \n",
       "                 45           46           47           48           49  \\\n",
       " count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000   \n",
       " mean      0.251244     0.042201     0.151398     0.110634     0.043510   \n",
       " std       0.032095     0.020304     0.047372     0.056237     0.019472   \n",
       " min       0.168496     0.011496     0.028061     0.011101     0.010377   \n",
       " 25%       0.232332     0.023597     0.122165     0.066971     0.026541   \n",
       " 50%       0.259593     0.041357     0.158317     0.118826     0.040434   \n",
       " 75%       0.276931     0.058516     0.186858     0.156496     0.060423   \n",
       " max       0.290151     0.079067     0.218336     0.196004     0.079772   \n",
       " \n",
       "                 50           51           52           53           54  \n",
       " count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000  \n",
       " mean      0.101082     0.094218     0.114990    13.504821    52.885670  \n",
       " std       0.054633     0.058128     0.043888     9.303333    29.367113  \n",
       " min       0.011100     0.011100     0.011100     0.000000     5.099020  \n",
       " 25%       0.058052     0.033142     0.084602     5.719505    29.778197  \n",
       " 50%       0.107728     0.102730     0.126339    12.369317    50.273392  \n",
       " 75%       0.145863     0.152218     0.153029    21.112237    69.094047  \n",
       " max       0.182511     0.174827     0.168289    35.777088   159.062881  \n",
       " \n",
       " [8 rows x 55 columns],\n",
       "                  0\n",
       " count  7100.000000\n",
       " mean      0.050658\n",
       " std       0.031272\n",
       " min       0.001000\n",
       " 25%       0.024458\n",
       " 50%       0.043468\n",
       " 75%       0.069216\n",
       " max       0.168000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this converts the panda dataframe into ndarray using function to_numpy\n",
    "x=df[['i_inj','j_inj','i_pro','j_pro','time',\n",
    "      'k1_inj','k2_inj','k3_inj','k4_inj','k5_inj','k6_inj',\n",
    "      'k7_inj','k8_inj','k9_inj','k10_inj','k11_inj','k12_inj',\n",
    "      'k1_pro','k2_pro','k3_pro','k4_pro','k5_pro','k6_pro',\n",
    "      'k7_pro','k8_pro','k9_pro','k10_pro','k11_pro','k12_pro',\n",
    "      'por1_inj','por2_inj','por3_inj','por4_inj','por5_inj','por6_inj',\n",
    "      'por7_inj','por8_inj','por9_inj','por10_inj','por11_inj','por12_inj',\n",
    "      'por1_pro','por2_pro','por3_pro','por4_pro','por5_pro','por6_pro',\n",
    "      'por7_pro','por8_pro','por9_pro','por10_pro','por11_pro','por12_pro',\n",
    "      'min_distance_inj','distance']].to_numpy() \n",
    "\n",
    "#same as above converted to nd array using to_numpy function\n",
    "y=df[['cum_co2']].to_numpy() \n",
    "\n",
    "pd.DataFrame(x).describe(),pd.DataFrame(y).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6270cd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>7100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.514255</td>\n",
       "      <td>0.576556</td>\n",
       "      <td>0.457766</td>\n",
       "      <td>0.597682</td>\n",
       "      <td>0.499762</td>\n",
       "      <td>0.363390</td>\n",
       "      <td>0.551650</td>\n",
       "      <td>0.694888</td>\n",
       "      <td>0.478037</td>\n",
       "      <td>0.447244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680187</td>\n",
       "      <td>0.454410</td>\n",
       "      <td>0.648207</td>\n",
       "      <td>0.538297</td>\n",
       "      <td>0.477462</td>\n",
       "      <td>0.524949</td>\n",
       "      <td>0.507664</td>\n",
       "      <td>0.660922</td>\n",
       "      <td>0.377471</td>\n",
       "      <td>0.310376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.234113</td>\n",
       "      <td>0.213935</td>\n",
       "      <td>0.232674</td>\n",
       "      <td>0.225511</td>\n",
       "      <td>0.292871</td>\n",
       "      <td>0.406748</td>\n",
       "      <td>0.357945</td>\n",
       "      <td>0.332316</td>\n",
       "      <td>0.299362</td>\n",
       "      <td>0.282358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263818</td>\n",
       "      <td>0.300488</td>\n",
       "      <td>0.248963</td>\n",
       "      <td>0.304143</td>\n",
       "      <td>0.280599</td>\n",
       "      <td>0.318724</td>\n",
       "      <td>0.355029</td>\n",
       "      <td>0.279207</td>\n",
       "      <td>0.260036</td>\n",
       "      <td>0.190740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.428808</td>\n",
       "      <td>0.273936</td>\n",
       "      <td>0.428808</td>\n",
       "      <td>0.242254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207456</td>\n",
       "      <td>0.436862</td>\n",
       "      <td>0.210355</td>\n",
       "      <td>0.173097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524735</td>\n",
       "      <td>0.179088</td>\n",
       "      <td>0.494571</td>\n",
       "      <td>0.302157</td>\n",
       "      <td>0.232927</td>\n",
       "      <td>0.273913</td>\n",
       "      <td>0.134627</td>\n",
       "      <td>0.467601</td>\n",
       "      <td>0.159865</td>\n",
       "      <td>0.160292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.515957</td>\n",
       "      <td>0.605960</td>\n",
       "      <td>0.457447</td>\n",
       "      <td>0.639073</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.155986</td>\n",
       "      <td>0.549215</td>\n",
       "      <td>0.825989</td>\n",
       "      <td>0.440795</td>\n",
       "      <td>0.468305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748820</td>\n",
       "      <td>0.441923</td>\n",
       "      <td>0.684568</td>\n",
       "      <td>0.582604</td>\n",
       "      <td>0.433135</td>\n",
       "      <td>0.563722</td>\n",
       "      <td>0.559654</td>\n",
       "      <td>0.733122</td>\n",
       "      <td>0.345733</td>\n",
       "      <td>0.293409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.712766</td>\n",
       "      <td>0.743377</td>\n",
       "      <td>0.640957</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>0.756808</td>\n",
       "      <td>0.819311</td>\n",
       "      <td>0.958430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708521</td>\n",
       "      <td>0.726836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.891333</td>\n",
       "      <td>0.695869</td>\n",
       "      <td>0.834566</td>\n",
       "      <td>0.786330</td>\n",
       "      <td>0.721175</td>\n",
       "      <td>0.786200</td>\n",
       "      <td>0.861913</td>\n",
       "      <td>0.902917</td>\n",
       "      <td>0.590105</td>\n",
       "      <td>0.415650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000   \n",
       "mean      0.514255     0.576556     0.457766     0.597682     0.499762   \n",
       "std       0.234113     0.213935     0.232674     0.225511     0.292871   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.351064     0.428808     0.273936     0.428808     0.242254   \n",
       "50%       0.515957     0.605960     0.457447     0.639073     0.500000   \n",
       "75%       0.712766     0.743377     0.640957     0.768212     0.756808   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000  ...   \n",
       "mean      0.363390     0.551650     0.694888     0.478037     0.447244  ...   \n",
       "std       0.406748     0.357945     0.332316     0.299362     0.282358  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.207456     0.436862     0.210355     0.173097  ...   \n",
       "50%       0.155986     0.549215     0.825989     0.440795     0.468305  ...   \n",
       "75%       0.819311     0.958430     1.000000     0.708521     0.726836  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "                45           46           47           48           49  \\\n",
       "count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000   \n",
       "mean      0.680187     0.454410     0.648207     0.538297     0.477462   \n",
       "std       0.263818     0.300488     0.248963     0.304143     0.280599   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.524735     0.179088     0.494571     0.302157     0.232927   \n",
       "50%       0.748820     0.441923     0.684568     0.582604     0.433135   \n",
       "75%       0.891333     0.695869     0.834566     0.786330     0.721175   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                50           51           52           53           54  \n",
       "count  7100.000000  7100.000000  7100.000000  7100.000000  7100.000000  \n",
       "mean      0.524949     0.507664     0.660922     0.377471     0.310376  \n",
       "std       0.318724     0.355029     0.279207     0.260036     0.190740  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.273913     0.134627     0.467601     0.159865     0.160292  \n",
       "50%       0.563722     0.559654     0.733122     0.345733     0.293409  \n",
       "75%       0.786200     0.861913     0.902917     0.590105     0.415650  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scale the training data\n",
    "scaler=MinMaxScaler()\n",
    "train_data=scaler.fit_transform(x) # it is important to fit the scaler into training set only\n",
    "pd.DataFrame(train_data).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "558f4316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.050658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.031272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.024458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.043468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.069216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  7100.000000\n",
       "mean      0.050658\n",
       "std       0.031272\n",
       "min       0.001000\n",
       "25%       0.024458\n",
       "50%       0.043468\n",
       "75%       0.069216\n",
       "max       0.168000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets=y\n",
    "\n",
    "pd.DataFrame(train_targets).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0191f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(params):\n",
    "    #convert the best solution into a layer sizes, integer values from floats\n",
    "    # transform the layer sizes from float (possibly negative) values into hiddenLayerSizes tuple:\n",
    "    if round(params[1]) <= 9:\n",
    "        hiddenLayerSizes = round(params[0]),\n",
    "    elif round(params[2]) <= 9:\n",
    "        hiddenLayerSizes = (round(params[0]), round(params[1]))\n",
    "    elif round(params[3]) <= 9:\n",
    "        hiddenLayerSizes = (round(params[0]), round(params[1]), round(params[2]))\n",
    "    elif round(params[4]) <= 9:\n",
    "        hiddenLayerSizes = (round(params[0]), round(params[1]), round(params[2]), round(params[3]))\n",
    "    else :\n",
    "        hiddenLayerSizes = (round(params[0]), round(params[1]), round(params[2]), round(params[3]), \n",
    "                            round(params[4]))\n",
    "\n",
    "    return hiddenLayerSizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b204eb10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(individuals):\n",
    "    # transform the layer sizes from float (possibly negative) values into hiddenLayerSizes tuple:\n",
    "    hiddenLayerSizes=converter(individuals)\n",
    "    #print (hiddenLayerSizes,len(hiddenLayerSizes))\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayerSizes[0],input_dim=train_data.shape[1],activation='relu'))\n",
    "    \n",
    "    \n",
    "    if len(hiddenLayerSizes)>1:\n",
    "        for i in range(len(hiddenLayerSizes)-1):\n",
    "            model.add(keras.layers.Dense(hiddenLayerSizes[i+1],activation='relu')) #hidden layer generator loop\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "        \n",
    "    learning_rate=1e-3\n",
    "    #print (learning_rate)\n",
    "\n",
    "    \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.compile(loss='mse', metrics=['mae'],\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate))  \n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c544ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_round_generator(params):\n",
    "    rounded_ind = (converter(params))\n",
    "    return rounded_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7b9275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_validation(individual):\n",
    "\n",
    "    num_epochs = 20\n",
    "    k = 4\n",
    "    num_val_samples = len(train_data) // k\n",
    "    #print('num_val_samples:',num_val_samples)\n",
    "    all_scores = [] \n",
    "    \n",
    "    batch_size=32\n",
    "    #rint ('batch_size',batch_size)\n",
    "    \n",
    "    for i in range(k):\n",
    "        #print(f\"Processing fold #{i}\")    \n",
    "        val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]     \n",
    "        val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]    \n",
    "        partial_train_data = np.concatenate([train_data[:i * num_val_samples],         \n",
    "                                                 train_data[(i + 1) * num_val_samples:]],axis=0)    \n",
    "        partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],         \n",
    "                                                    train_targets[(i + 1) * num_val_samples:]],axis=0)   \n",
    "        \n",
    "        model = build_model(individual)\n",
    "    \n",
    "        history = model.fit(partial_train_data, partial_train_targets,validation_data=(val_data, val_targets),\n",
    "                            epochs=num_epochs, batch_size=batch_size, verbose=0, callbacks=None)  \n",
    "        \n",
    "        val_mae_loss, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "        all_scores.append(val_mae)\n",
    "        \n",
    "    f=np.mean(all_scores)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24c99ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process began\n",
      "gen\tnevals\tmin      \tavg      \n",
      "0  \t30    \t0.0177838\t0.0345981\n",
      "Processing gen #1\n",
      "1  \t24    \t0.0177838\t0.0263311\n",
      "# of layers: 5 best_individual (99, 69, 25, 11, 38) Fitness (0.017783766612410545,)\n",
      "Processing gen #2\n",
      "2  \t24    \t0.0177838\t0.0230732\n",
      "# of layers: 5 best_individual (99, 69, 25, 11, 38) Fitness (0.017783766612410545,)\n",
      "Processing gen #3\n",
      "3  \t28    \t0.0172224\t0.0212471\n",
      "# of layers: 5 best_individual (99, 69, 25, 11, 38) Fitness (0.017783766612410545,)\n",
      "Processing gen #4\n",
      "4  \t28    \t0.0160954\t0.0204032\n",
      "# of layers: 5 best_individual (99, 35, 39, 10, 81) Fitness (0.017222432186827064,)\n",
      "Processing gen #5\n",
      "5  \t27    \t0.0160954\t0.0195613\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 38) Fitness (0.016095392871648073,)\n",
      "Processing gen #6\n",
      "6  \t28    \t0.0160954\t0.0202702\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 38) Fitness (0.016095392871648073,)\n",
      "Processing gen #7\n",
      "7  \t27    \t0.0160182\t0.0179597\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 38) Fitness (0.016095392871648073,)\n",
      "Processing gen #8\n",
      "8  \t28    \t0.0160182\t0.0183431\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #9\n",
      "9  \t27    \t0.0160182\t0.0186287\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #10\n",
      "10 \t24    \t0.0160182\t0.0185277\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #11\n",
      "11 \t28    \t0.0160182\t0.0191885\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #12\n",
      "12 \t22    \t0.0160182\t0.0185068\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #13\n",
      "13 \t25    \t0.0160182\t0.018161 \n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #14\n",
      "14 \t26    \t0.0160182\t0.0180498\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #15\n",
      "15 \t28    \t0.0160182\t0.0185101\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #16\n",
      "16 \t27    \t0.0160182\t0.0188875\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #17\n",
      "17 \t28    \t0.0160182\t0.0181714\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #18\n",
      "18 \t28    \t0.0160182\t0.0181799\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #19\n",
      "19 \t26    \t0.0160182\t0.0180528\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #20\n",
      "20 \t26    \t0.0160182\t0.0195522\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #21\n",
      "21 \t27    \t0.0160182\t0.0192945\n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #22\n",
      "22 \t25    \t0.0152779\t0.017922 \n",
      "# of layers: 5 best_individual (97, 69, 28, 11, 38) Fitness (0.016018207650631666,)\n",
      "Processing gen #23\n",
      "23 \t25    \t0.0152779\t0.0182375\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #24\n",
      "24 \t26    \t0.0152779\t0.0183333\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #25\n",
      "25 \t26    \t0.0152779\t0.0185163\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #26\n",
      "26 \t24    \t0.0152779\t0.0181573\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #27\n",
      "27 \t27    \t0.0152779\t0.0185941\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #28\n",
      "28 \t24    \t0.0152779\t0.0183148\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #29\n",
      "29 \t23    \t0.0152779\t0.0181551\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #30\n",
      "30 \t27    \t0.0152779\t0.0185061\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #31\n",
      "31 \t28    \t0.0152779\t0.0190537\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #32\n",
      "32 \t26    \t0.0152779\t0.0190757\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #33\n",
      "33 \t25    \t0.0152779\t0.0176341\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #34\n",
      "34 \t24    \t0.0152779\t0.018813 \n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #35\n",
      "35 \t25    \t0.0152779\t0.018048 \n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #36\n",
      "36 \t27    \t0.0152779\t0.0189776\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #37\n",
      "37 \t26    \t0.0152779\t0.0182634\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #38\n",
      "38 \t24    \t0.0152779\t0.0185096\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #39\n",
      "39 \t28    \t0.0152779\t0.0186103\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #40\n",
      "40 \t24    \t0.0152779\t0.0183814\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #41\n",
      "41 \t26    \t0.0152779\t0.0184406\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #42\n",
      "42 \t25    \t0.0152779\t0.0189068\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #43\n",
      "43 \t24    \t0.0152779\t0.0180199\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #44\n",
      "44 \t23    \t0.0152779\t0.017798 \n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #45\n",
      "45 \t24    \t0.0152779\t0.0180166\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #46\n",
      "46 \t28    \t0.0152779\t0.0183712\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #47\n",
      "47 \t27    \t0.0152779\t0.0183113\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #48\n",
      "48 \t28    \t0.0152779\t0.0180328\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #49\n",
      "49 \t25    \t0.0152779\t0.0181527\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "Processing gen #50\n",
      "50 \t28    \t0.0150903\t0.0182808\n",
      "# of layers: 5 best_individual (99, 79, 41, 10, 49) Fitness (0.015277909813448787,)\n",
      "- GA Best solution before converting is:  [99.13722183661845, 72.70994927890641, 62.537770743789835, 9.899333899460105, 41.34852966086884] ,loss is: 0.015090323518961668\n",
      "- GA Best solution after converting - Layers and unit numbers:  (99, 73, 63, 10, 41) , Loss =  0.015090323518961668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    print('The process began')\n",
    "    \n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "    \n",
    "    #iterator is required to iterate in elements of tuple logbook.select(\"max\")\n",
    "    iterator=0\n",
    "    #create a numpy array for later use as container and write best individuals to csv file\n",
    "    all_in_one=np.zeros([1,7])\n",
    "    \n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "        \n",
    "        print(f\"Processing gen #{gen}\") \n",
    "            \n",
    "        \n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "            \n",
    "        #loop is for printing out the best ind and fitness values of each generation\n",
    "        for ind in population:\n",
    "           \n",
    "            if ind.fitness.values==logbook.select(\"min\")[iterator]: \n",
    "                print('# of layers:',len(ind_round_generator(ind)),'best_individual',ind_round_generator(ind),\n",
    "                      'Fitness',ind.fitness.values)\n",
    "                tuple1=tuple(ind) #convert individual into tuple\n",
    "                tuple2=tuple(ind.fitness.values) #convert individual fitness into tuple\n",
    "                tuple3=(gen-1,)+tuple1+tuple2\n",
    "                #combine generation, individual, fitnesses into one tuple\n",
    "                to_array=np.asarray(tuple3)  #convert combined tuple into an array\n",
    "                reshaped=np.reshape(to_array,(1,7)) #reshape array\n",
    "                all_in_one=np.append(all_in_one,reshaped,axis=0) #add new values in a new row using axis=0 option\n",
    "                break #do not output all ind with highest fitness value\n",
    "        iterator+=1\n",
    " \n",
    "    return population, logbook\n",
    "\n",
    "\n",
    "# boundaries for layer size parameters: \n",
    "             # [0,     1     2,     3     4     5      \n",
    "BOUNDS_LOW =  [   10,    0,   -20,  -50,  -100 ]\n",
    "              \n",
    "BOUNDS_HIGH = [   100,  100,  100,   100,  100]\n",
    "\n",
    "\n",
    "NUM_OF_PARAMS = len(BOUNDS_HIGH)\n",
    "\n",
    "# Genetic Algorithm constants:\n",
    "POPULATION_SIZE = 30\n",
    "P_CROSSOVER = 0.9  # probability for crossover\n",
    "P_MUTATION = 0.3   # probability for mutating an individual\n",
    "MAX_GENERATIONS = 50\n",
    "HALL_OF_FAME_SIZE = 2\n",
    "CROWDING_FACTOR = 10.0  # crowding factor for crossover and mutation\n",
    "\n",
    "# set the random seed:\n",
    "RANDOM_SEED = 44\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# define a single objective, maximizing fitness strategy:\n",
    "creator.create(\"FitnessMax_tuning_model_a\", base.Fitness, weights=(-1.0,))\n",
    "\n",
    "# create the Individual class based on list:\n",
    "creator.create(\"Individual_tuning_model_a\", list, \n",
    "               fitness=creator.FitnessMax_tuning_model_a)\n",
    "\n",
    "# define the layer_size_attributes individually:\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    # \"layer_size_attribute_0\", \"layer_size_attribute_1\", ...\n",
    "    toolbox.register(\"layer_size_attribute_\" + str(i),\n",
    "                     random.uniform,\n",
    "                     BOUNDS_LOW[i],\n",
    "                     BOUNDS_HIGH[i])\n",
    "\n",
    "# create a tuple containing an layer_size_attribute generator for each hidden layer:\n",
    "layer_size_attributes = ()\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    layer_size_attributes = layer_size_attributes + \\\n",
    "                            (toolbox.__getattribute__(\"layer_size_attribute_\" + str(i)),)\n",
    "\n",
    "# create the individual operator to fill up an Individual instance:\n",
    "toolbox.register(\"individualCreator\",\n",
    "                 tools.initCycle,\n",
    "                 creator.Individual_tuning_model_a,\n",
    "                 layer_size_attributes,\n",
    "                 n=1)\n",
    "\n",
    "# create the population operator to generate a list of individuals:\n",
    "toolbox.register(\"populationCreator\",\n",
    "                 tools.initRepeat,\n",
    "                 list,\n",
    "                 toolbox.individualCreator)\n",
    "\n",
    "\n",
    "# fitness calculation\n",
    "def accuracy(individual):\n",
    "    fitness=k_fold_validation(individual)\n",
    "    #print (type(fitness))\n",
    "    return fitness,\n",
    "\n",
    "\n",
    "toolbox.register(\"evaluate\", accuracy)\n",
    "\n",
    "# genetic operators:mutFlipBit\n",
    "\n",
    "# genetic operators:\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=2)\n",
    "\n",
    "toolbox.register(\"mate\",\n",
    "                 tools.cxSimulatedBinaryBounded,\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR)\n",
    "\n",
    "toolbox.register(\"mutate\",\n",
    "                 tools.mutPolynomialBounded,\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR,\n",
    "                 indpb=1.0/NUM_OF_PARAMS)\n",
    "\n",
    "\n",
    "# Genetic Algorithm flow:\n",
    "def main():\n",
    "\n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.populationCreator(n=POPULATION_SIZE)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # perform the Genetic Algorithm flow with hof feature added:\n",
    "    population, logbook = eaSimpleWithElitism(population,\n",
    "                                                      toolbox,\n",
    "                                                      cxpb=P_CROSSOVER,\n",
    "                                                      mutpb=P_MUTATION,\n",
    "                                                      ngen=MAX_GENERATIONS,\n",
    "                                                      stats=stats,\n",
    "                                                      halloffame=hof,\n",
    "                                                      verbose=True)\n",
    "    #print all of the best solutions found:    \n",
    "       \n",
    "    print(\"- GA Best solution before converting is: \",hof.items[0],\",loss is:\", hof.items[0].fitness.values[0])\n",
    "    print(\"- GA Best solution after converting - Layers and unit numbers: \",converter(hof.items[0]),\n",
    "          \", Loss = \", hof.items[0].fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues = logbook.select(\"min\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='red')\n",
    "    plt.plot(meanFitnessValues, color='green')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Min / Average Fitness')\n",
    "    plt.title('Min and Average fitness over Generations')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#execute the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd1ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
