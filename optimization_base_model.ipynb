{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9b0dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import the libary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import statistics\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from math import sqrt\n",
    "import math\n",
    "\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "#read the data\n",
    "df=pd.read_csv('thesis_sample_100_shuffled_w_respect_to_id_Base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54750e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this converts the panda dataframe into ndarray using function to_numpy\n",
    "x=df[['i_inj','j_inj','i_pro','j_pro','time']].to_numpy() \n",
    "\n",
    "#same as above converted to nd array using to_numpy function\n",
    "y=df[['cum_co2']].to_numpy() \n",
    "\n",
    "pd.DataFrame(x).describe(),pd.DataFrame(y).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and prepare the test dataset \n",
    "test_last_timesteps=pd.read_csv('thesis_testing_data_last_timesteps.csv')\n",
    "test_last_timesteps=test_last_timesteps.dropna()\n",
    "test_last_timesteps_data=test_last_timesteps[['i_inj','j_inj','i_pro','j_pro','time']].to_numpy()\n",
    "\n",
    "test_last_timesteps_actual=test_last_timesteps[['cum_co2']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data using minmax\n",
    "scaler=MinMaxScaler()\n",
    "train_data=scaler.fit_transform(x) # it is important to fit the scaler into training set only\n",
    "pd.DataFrame(train_data).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the testing data \n",
    "test_last_timesteps_data=scaler.transform(test_last_timesteps_data)\n",
    "pd.DataFrame(test_last_timesteps_data).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets=y\n",
    "\n",
    "pd.DataFrame(train_targets).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(params):\n",
    "    #convert the best solution into a layer sizes, integer values from floats\n",
    "    # transform the layer sizes from float (possibly negative) values into hiddenLayerSizes tuple:\n",
    "    if round(params[3]) <= 0:\n",
    "        hiddenLayerSizes = round(params[2]),\n",
    "    elif round(params[4]) <= 0:\n",
    "        hiddenLayerSizes = (round(params[2]), round(params[3]))\n",
    "    elif round(params[5]) <= 0:\n",
    "        hiddenLayerSizes = (round(params[2]), round(params[3]), round(params[4]))\n",
    "    elif round(params[6]) <= 0:\n",
    "        hiddenLayerSizes = (round(params[2]), round(params[3]), round(params[4]), round(params[5]) )\n",
    "    elif round(params[7]) <= 0:\n",
    "        hiddenLayerSizes = (round(params[2]), round(params[3]), round(params[4]), round(params[5]),\n",
    "                           round(params[6]))\n",
    "    elif round(params[8]) <= 0:\n",
    "        hiddenLayerSizes = (round(params[2]), round(params[3]), round(params[4]), round(params[5]),\n",
    "                           round(params[6]),round(params[7]))\n",
    "    elif round(params[9]) <= 0:\n",
    "        hiddenLayerSizes = (round(params[2]), round(params[3]), round(params[4]), round(params[5]),\n",
    "                           round(params[6]),round(params[7]),round(params[8]))\n",
    "    elif round(params[10]) <= 0:\n",
    "        hiddenLayerSizes = (round(params[2]), round(params[3]), round(params[4]), round(params[5]),\n",
    "                           round(params[6]),round(params[7]),round(params[8]),round(params[9]))\n",
    "    elif round(params[11]) <= 0:\n",
    "        hiddenLayerSizes = (round(params[2]), round(params[3]), round(params[4]), round(params[5]),\n",
    "                           round(params[6]),round(params[7]),round(params[8]),round(params[9]),\n",
    "                           round(params[10]))\n",
    "    else :\n",
    "        hiddenLayerSizes = (round(params[2]), round(params[3]), round(params[4]), round(params[5]),\n",
    "                           round(params[6]),round(params[7]),round(params[8]),round(params[9]),\n",
    "                           round(params[10]),round(params[11]))\n",
    "\n",
    "    return hiddenLayerSizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204eb10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(individuals):\n",
    "    # transform the layer sizes from float (possibly negative) values into hiddenLayerSizes tuple:\n",
    "    hiddenLayerSizes=converter(individuals)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayerSizes[0],input_dim=train_data.shape[1],activation='relu'))\n",
    "    \n",
    "    \n",
    "    if len(hiddenLayerSizes)>1:\n",
    "        for i in range(len(hiddenLayerSizes)-1):\n",
    "            model.add(keras.layers.Dense(hiddenLayerSizes[i+1],activation='relu')) #hidden layer generator loop\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    learning_rate=individuals[1]\n",
    "    \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.compile(loss='mse', metrics=['mae'],\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate))  \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1535419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#optimal architecture\n",
    "\n",
    "parameters=[ 32, 1e-03,\n",
    "             96, 24, 90, 67, 60,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec650f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_validation(individual):\n",
    " \n",
    "    num_epochs = 2000\n",
    "    k = 4\n",
    "    num_val_samples = len(train_data) // k\n",
    "    print('num_val_samples:',num_val_samples)\n",
    "    all_scores = []\n",
    "    all_mae_histories = []\n",
    "    \n",
    "    batch_size=round(individual[0])\n",
    "    print ('batch_size:',batch_size)\n",
    "    \n",
    "    for i in range(k):\n",
    "        print(f\"Processing fold #{i}\")    \n",
    "        val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]     \n",
    "        val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]    \n",
    "        partial_train_data = np.concatenate([train_data[:i * num_val_samples],         \n",
    "                                                 train_data[(i + 1) * num_val_samples:]],axis=0)    \n",
    "        partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],         \n",
    "                                                    train_targets[(i + 1) * num_val_samples:]],axis=0)   \n",
    "        \n",
    "        model = build_model(individual)\n",
    "    \n",
    "        history = model.fit(partial_train_data, partial_train_targets,validation_data=(val_data, val_targets),\n",
    "                            epochs=num_epochs, batch_size=batch_size, verbose=2, callbacks=None)  \n",
    "        \n",
    "        val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "        all_scores.append(val_mae)\n",
    "        \n",
    "        mae_history = history.history[\"val_mae\"]    \n",
    "        all_mae_histories.append(mae_history)\n",
    "        \n",
    "    f=np.mean(all_scores)\n",
    "    all_mae_histories\n",
    "    return batch_size, num_epochs, all_mae_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491a9cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size, num_epochs, all_mae_histories= k_fold_validation(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code return the average of scores at each epoch among all validation scores\n",
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec568f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting full mae history\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting truncated MAE history\n",
    "truncated_mae_history = average_mae_history[700:15000]\n",
    "plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ebc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the epoch number of lowest mean average MAE\n",
    "min_mean_average_mae=np.amin(average_mae_history)\n",
    "best_epoch_number=average_mae_history.index(min_mean_average_mae)+1\n",
    "print ('best_epoch_number:',best_epoch_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159efe84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train the model with best epoch number\n",
    "model = build_model(parameters)\n",
    "history= model.fit(train_data, train_targets,epochs=best_epoch_number,\n",
    "          batch_size=batch_size, verbose=2)\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc341f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this saves the model that is used to generate the figures for thesis\n",
    "\n",
    "model.save('Base_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a7d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads the model you just saved above\n",
    "\n",
    "model_saved = tf.keras.models.load_model(\"Base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc71c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will plot crossplots\n",
    "\n",
    "def plot(y,ypred):\n",
    "    \n",
    "    #the following will get the shape of the input ytest or ypred\n",
    "    x=([len(l) for l in ypred])[1]\n",
    "    for i in range(0,x):\n",
    "        plt.rcParams[\"figure.figsize\"] = (5,5) #change the figure size here\n",
    "        a = plt.axes(aspect='equal')\n",
    "        plt.scatter(y[:,i], ypred[:,i],marker='s',color='b')\n",
    "        plt.xlabel('Cum. CO2 simulation (1e9 m3)')\n",
    "        plt.ylabel('Cum. CO2 ANN (1e9 m3)')\n",
    "        ls = [0, np.amax(y[:,i])]\n",
    "        plt.xlim(ls)\n",
    "        plt.ylim(ls)\n",
    "        plt.plot(ls, ls, color='red')\n",
    "        plt.suptitle('Base Model')\n",
    "        plt.savefig('figure_cross_plot_base_model.png',dpi=330, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "def metrcs(y,ypred):\n",
    "    #the following will get the shape of the input ytest or ypred\n",
    "    x=([len(a) for a in ypred])[1]\n",
    "    \n",
    "    for i in range(0,x):\n",
    "        MAE=metrics.mean_absolute_error(y[:,i],ypred[:,i])\n",
    "        R2=metrics.r2_score(y[:,i],ypred[:,i])\n",
    "        MAPE=metrics.mean_absolute_percentage_error(y[:,i],ypred[:,i])\n",
    "        print ('MAE=',MAE,'R2=',R2, \"MAPE=\",MAPE)\n",
    "        \n",
    "def relative_error(y,ypred):\n",
    "    \n",
    "    x=([len(l) for l in ypred])[1]\n",
    "    for i in range(0,x):\n",
    "        error=abs(y[:,i]-ypred[:,i])/y[:,i]\n",
    "        min_error=np.amin(error)\n",
    "        mean=np.mean(error)\n",
    "        median=np.median(error)\n",
    "        max_error=np.amax(error)\n",
    "        std_dev=statistics.stdev(error)\n",
    "        print('min=',min_error,'mean=',mean,'median=',median,'max=',max_error, \"std_dev=\",std_dev)\n",
    "        plt.scatter(np.arange(1,len(y)+1).reshape(len(y),1), error,marker='o', color='r',s=50)\n",
    "        plt.xlabel('Test data points')\n",
    "        plt.ylabel('Absolute relative error')\n",
    "        lsy=[min_error, max_error]\n",
    "        lsx=[0,len(y)+1]\n",
    "        plt.xlim(lsx)\n",
    "        plt.ylim(lsy)\n",
    "        plt.grid(False)\n",
    "        plt.suptitle('Base Model')\n",
    "        plt.savefig('figure_mean_rel_error_per_experiment_base_Model.png',dpi=330, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.figure(5)\n",
    "        plt.suptitle('Base Model')\n",
    "        plt.hist(error, bins=5)\n",
    "        plt.xlabel('Prediction Error')\n",
    "        plt.xlim([min_error, max_error])\n",
    "        plt.ylabel('Count')\n",
    "        plt.grid(False)    \n",
    "        plt.savefig('figure_histogram_base_model.png',dpi=330, bbox_inches='tight')\n",
    "\n",
    "def mean_relative_error(y,ypred):\n",
    "    container=np.array([])\n",
    "    x=([len(l) for l in ypred])[1]\n",
    "    for i in range(0,x):\n",
    "        error=abs(y[:,i]-ypred[:,i])/y[:,i]\n",
    "        #this outer loop will loop over the timesteps\n",
    "        for timestep in range(71):  \n",
    "            #datasets contains 71 timestep each. inner loop will extract the same timesteps for all dataset\n",
    "            #for ex: there are 20 data for testing dataset which have a timestep of 1\n",
    "            for dataset in range(0,len(error),71): \n",
    "                container=np.append(container,error[timestep+dataset])\n",
    "        mean_rel_err=np.array([])\n",
    "        #this loop is for taking the average of the error at each timesteps\n",
    "        #step size will be the total number of datapoints in each timestep. ex: for 20 dataset step size will be 20\n",
    "        for i in range(0,len(container),int(len(container)/71.0)): \n",
    "            mean_rel_err=np.append(mean_rel_err,np.mean(container[i:i+int(len(container)/71.0),]))\n",
    "        plt.figure(6)\n",
    "        plt.scatter(np.arange(1,72,1),mean_rel_err,c='r')\n",
    "        plt.xlabel('Timestep number')\n",
    "        plt.ylabel('Mean relative error')\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "        \n",
    "def mean_rel_error_per_experiment(y,ypred): #this function plots the mean error values with respect to experiment id \n",
    "    x=([len(l) for l in ypred])[1]\n",
    "    for i in range(0,x):\n",
    "        mean_rel_err=np.array([])\n",
    "        error=abs(y[:,i]-ypred[:,i])/y[:,i]\n",
    "        for i in range(0,len(error),71): \n",
    "            mean_rel_err=np.append(mean_rel_err,np.mean(error[i:i+71]))\n",
    "        \n",
    "        plt.figure(7)\n",
    "        plt.scatter(np.arange(1,len(error)/71+1,1),mean_rel_err,c='r')\n",
    "        plt.xlabel('Experiment number')\n",
    "        plt.ylabel('Mean relative error')\n",
    "        plt.xticks(np.arange(1,len(error)/71+1,3))\n",
    "        plt.suptitle('Base Model')\n",
    "        plt.grid(False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da45941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions for training data\n",
    "y_pred=model.predict(train_data)\n",
    "#crossplot\n",
    "plot(y,y_pred)\n",
    "#metrics for training data\n",
    "metrcs(y,y_pred)\n",
    "#error plots for training data\n",
    "relative_error(y,y_pred)\n",
    "mean_relative_error(y,y_pred)\n",
    "mean_rel_error_per_experiment(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the crossplot for testing dataset\n",
    "y_test_pred = model.predict(test_data)\n",
    "#crossplot\n",
    "plot(y_test_actual,y_test_pred)\n",
    "#metrics\n",
    "metrcs(y_test_actual,y_test_pred)\n",
    "#error plots\n",
    "relative_error(y_test_actual,y_test_pred)\n",
    "mean_relative_error(y_test_actual,y_test_pred)\n",
    "mean_rel_error_per_experiment(y_test_actual,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397ddbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#prediction of last timesteps\n",
    "y_test_last_timesteps_pred = model.predict(test_last_timesteps_data)\n",
    "#cross plot\n",
    "plot(test_last_timesteps_actual,y_test_last_timesteps_pred)\n",
    "#metrics\n",
    "metrcs(test_last_timesteps_actual,y_test_last_timesteps_pred)\n",
    "#error plots\n",
    "relative_error(test_last_timesteps_actual,y_test_last_timesteps_pred)\n",
    "mean_rel_error_per_experiment(test_last_timesteps_actual,y_test_last_timesteps_pred)\n",
    "\n",
    "#writing out the prediction and actual values\n",
    "#convert to dataframe\n",
    "actual=pd.DataFrame(test_last_timesteps_actual)\n",
    "predicted=pd.DataFrame(y_test_last_timesteps_pred)\n",
    "actual.columns=['actual_values']\n",
    "predicted.columns=['predicted_values']\n",
    "merged=pd.concat([actual, predicted], axis=1)\n",
    "merged.to_excel('actual_predicted_base_model.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b6e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is for reading the files. \n",
    "permxpor_file=pd.read_csv('perm_x_por_sum.csv')\n",
    "permxpor_file=permxpor_file[['i','j','permxpor']].to_numpy() \n",
    "boundary_grid_file=pd.read_csv('boundary_grid.csv')\n",
    "boundary_grid_file=boundary_grid_file[['I','J']].to_numpy()\n",
    "boundary_file=pd.read_csv('boundary_constraint.csv')\n",
    "boundary_file=boundary_file[['I','J','boundary_with_fault_zones']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d88f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generator functions start here!!!\"\"\"\n",
    "def boundary_constraint(params,boundary_file): \n",
    "    constraint = np.array([])\n",
    "    for out in range(0,3,2):  #out iterates between inj and pro grid numbers\n",
    "        for rows in range(len(boundary_file)):\n",
    "            if round(params[out+1])==boundary_file[rows,1]: #compare j values\n",
    "            #print (perm[rows])\n",
    "                if round(params[out])==boundary_file[rows,0]:\n",
    "                    constraint=np.append(constraint,boundary_file[rows,2])\n",
    "    \n",
    "    if sum(constraint)==2.0:\n",
    "        multiplier=1.0\n",
    "    else:\n",
    "        multiplier=0.7  #these are the fitness multiplier to penalize out of boundary individuals\n",
    "    \n",
    "    return multiplier\n",
    "\n",
    "def permxpor_generator(params,permxpor_file):\n",
    "    permxpor_container = np.array([]) #empty array used as a container of permxpor\n",
    "    for out in range(0,3,2):\n",
    "        #print(out)#out iterates between inj and pro grid numbers\n",
    "        for rows in range(len(permxpor_file)):\n",
    "            #print (rows) #len function is working properly\n",
    "            if round(params[out+1])==permxpor_file[rows,1]: #compare j values\n",
    "                #print (permxpor_file[rows])\n",
    "                if round(params[out])==permxpor_file[rows,0]: #compare i values\n",
    "                    permxpor_container=np.append(permxpor_container,\n",
    "                                                 permxpor_file[rows,2]) #return the matched permxpor value\n",
    "                    \n",
    "    return permxpor_container\n",
    "\n",
    "#min distance generator for inj grid address only, if you want to make for pro grid one change the params in formula \n",
    "def min_distance_generator(params, boundary_grid_file): \n",
    "    distance_container=np.array([])\n",
    "    for row in range(len(boundary_grid_file)):\n",
    "        distance=sqrt((boundary_grid_file[row,0]-round(params[0]))**2+(boundary_grid_file[row,1]-round(params[1]))**2)\n",
    "        distance_container=np.append(distance_container,distance)\n",
    "    min_distance=np.amin(distance_container)\n",
    "    \n",
    "    return min_distance\n",
    "\n",
    "def distance_generator(params):\n",
    "    distance=math.sqrt((round(params[0])-round(params[2]))**2+(round(params[1]-round(params[3])))**2)\n",
    "    return distance\n",
    "\n",
    "\"\"\"Normalizer function starts here!!!\"\"\"\n",
    "\n",
    "def normalizer(params):\n",
    "    i_inj=(round(params[0])- 1.00)/(95.0- 1.00)\n",
    "    j_inj=(round(params[1])- 1.00)/(152.00- 1.00)\n",
    "    i_pro=(round(params[2])- 1.00)/(95.0- 1.00)\n",
    "    j_pro=(round(params[3])- 1.00)/(152.00- 1.00)\n",
    "    time=1.00\n",
    "    params=np.array([i_inj,j_inj,i_pro,j_pro,time])\n",
    "    return params\n",
    "\n",
    "'''update the min max values'''\n",
    "#           'i_inj',     'j_inj',       'i_pro',       j_pro'     'time'\n",
    "#                0            1            2            3            4   \n",
    "# min       1.000000     1.000000     1.000000     1.000000    31.000000      \n",
    "# max      95.000000   152.000000    95.000000   152.000000  2161.000000   \n",
    "\n",
    "    \n",
    "def permxpor_normalizer(perms):\n",
    "    perms[0]=(perms[0]-  41.106744 )/( 394.800930-  41.106744 ) \n",
    "    perms[1]=(perms[1]- 38.693733)/( 427.480096- 38.693733)\n",
    "    \n",
    "    return perms,\n",
    "\n",
    "def min_distance_normalizer(min_distance):\n",
    "    min_distance=(min_distance-0.000000)/( 35.777088-0.000000)\n",
    "    return min_distance\n",
    "\n",
    "def distance_normalizer(distance):\n",
    "    distance=(distance-5.099020)/(159.062881-5.099020)\n",
    "    return distance\n",
    "\n",
    "'''update the min max values'''\n",
    "#'perm_x_por_inj_sum','perm_x_por_pro_sum','min_distance_inj','distance'\n",
    "#                  5            6            7            8    \n",
    "# min      41.106744    38.693733     0.000000     5.099020  \n",
    "# max     394.800930   427.480096    35.777088   159.062881  \n",
    "\n",
    "def converter_2(fitness_value):\n",
    "    cum_co2=fitness_value*10**9\n",
    "    return cum_co2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c94114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the results from this optimization run were used in manuscript results\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import seaborn as sns\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    print('The process began')\n",
    "    \n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "    \n",
    "\n",
    "    #iterator is required to iterate in elements of tuple logbook.select(\"max\")\n",
    "    iterator=0\n",
    "    #create a numpy array for later use as container and write best individuals to csv file\n",
    "    all_in_one=np.zeros([1,6])\n",
    "    all_individuals=np.zeros([1,6])\n",
    "    \n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "        \n",
    "        #loop is for printing out the best ind and fitness values of each generation\n",
    "        for ind in population:\n",
    "            \n",
    "            #here i want to print out all the individuals during the optimization\n",
    "            tuple_ind=tuple(ind)\n",
    "            tuple_fitness=tuple(ind.fitness.values)\n",
    "            tuples_to_array=np.asarray((gen,)+tuple_ind+tuple_fitness)\n",
    "            all_individuals=np.append(all_individuals,tuples_to_array.reshape(1,6),axis=0)\n",
    "            \n",
    "            if ind.fitness.values==logbook.select(\"max\")[iterator]: \n",
    "                print('best_individual',ind,\n",
    "                      'Fitness',ind.fitness.values)\n",
    "                tuple1=tuple(ind) #convert individual into tuple\n",
    "                tuple2=tuple(ind.fitness.values) #convert individual fitness into tuple\n",
    "                tuple3=(gen-1,)+tuple1+tuple2 #combine generation, individual, fitnesses into one tuple\n",
    "                to_array=np.asarray(tuple3)  #convert combined tuple into an array\n",
    "                reshaped=np.reshape(to_array,(1,6)) #reshape array\n",
    "                all_in_one=np.append(all_in_one,reshaped,axis=0) #add new values in a new row using axis=0 option\n",
    "                break #print only one ind per best fitness\n",
    "        iterator+=1\n",
    "        \n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "    \n",
    "    return population, logbook\n",
    "\n",
    "\n",
    "# boundaries well grid block addresses:\n",
    "            # [i_inj, j_inj, i_pro, j_pro]\n",
    "BOUNDS_LOW =  [1,       1,     1,      1]\n",
    "BOUNDS_HIGH = [95,     152,    95,   152]\n",
    "\n",
    "NUM_OF_PARAMS = len(BOUNDS_HIGH)\n",
    "\n",
    "# Genetic Algorithm constants:\n",
    "POPULATION_SIZE = 200\n",
    "P_CROSSOVER = 0.9  # probability for crossover\n",
    "P_MUTATION = 0.3   # probability for mutating an individual\n",
    "MAX_GENERATIONS = 500\n",
    "HALL_OF_FAME_SIZE = 3\n",
    "CROWDING_FACTOR = 10.0  # crowding factor for crossover and mutation\n",
    "\n",
    "# set the random seed:\n",
    "RANDOM_SEED = 44\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# define a single objective, maximizing fitness strategy:\n",
    "creator.create(\"FitnessMax_base_model\", base.Fitness, weights=(1.0,))\n",
    "\n",
    "# create the Individual class based on list:\n",
    "creator.create(\"Individual_base_model\", list,\n",
    "               fitness=creator.FitnessMax_base_model)\n",
    "\n",
    "# define the well gridblock addresses individually:\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    # \"block_address_0\", \"block_address_1\", ...\n",
    "    toolbox.register(\"block_address_\" + str(i),\n",
    "                     random.uniform,\n",
    "                     BOUNDS_LOW[i],\n",
    "                     BOUNDS_HIGH[i])\n",
    "\n",
    "# create a tuple containing an block address generator for each well i-j grid:\n",
    "block_addresses = ()\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    block_addresses = block_addresses + \\\n",
    "                            (toolbox.__getattribute__(\"block_address_\" + str(i)),)\n",
    "\n",
    "# create the individual operator to fill up an Individual instance:\n",
    "toolbox.register(\"individualCreator\",\n",
    "                 tools.initCycle,\n",
    "                 creator.Individual_base_model,\n",
    "                 block_addresses,\n",
    "                 n=1)\n",
    "\n",
    "# create the population operator to generate a list of individuals:\n",
    "toolbox.register(\"populationCreator\",\n",
    "                 tools.initRepeat,\n",
    "                 list,\n",
    "                 toolbox.individualCreator)\n",
    "\n",
    "\n",
    "# fitness calculation\n",
    "def accuracy(individual):\n",
    "    multiplier=boundary_constraint(individual,boundary_file)\n",
    "\n",
    "    #normalize all values\n",
    "    individual=normalizer(individual)    \n",
    "\n",
    "    #reshape individual for ANN model\n",
    "    individual=np.reshape(individual,(1,5))\n",
    "    \n",
    "    #evaluate the fitness function\n",
    "    accuracy= model(individual)\n",
    "    fitness=np.array(accuracy[:,0])*multiplier\n",
    "    \n",
    "    \n",
    "    return fitness,\n",
    "\n",
    "\n",
    "toolbox.register(\"evaluate\", accuracy)\n",
    "\n",
    "# genetic operators:\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=2)\n",
    "\n",
    "toolbox.register(\"mate\",\n",
    "                 tools.cxSimulatedBinaryBounded,\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR)\n",
    "\n",
    "toolbox.register(\"mutate\",\n",
    "                 tools.mutPolynomialBounded,\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR,\n",
    "                 indpb=1.0/NUM_OF_PARAMS)\n",
    "\n",
    "\n",
    "# Genetic Algorithm flow:\n",
    "def main():\n",
    "\n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.populationCreator(n=POPULATION_SIZE)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"max\", np.max)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # perform the Genetic Algorithm flow with hof feature added:\n",
    "    population, logbook = eaSimpleWithElitism(population,\n",
    "                                                      toolbox,\n",
    "                                                      cxpb=P_CROSSOVER,\n",
    "                                                      mutpb=P_MUTATION,\n",
    "                                                      ngen=MAX_GENERATIONS,\n",
    "                                                      stats=stats,\n",
    "                                                      halloffame=hof,\n",
    "                                                      verbose=True)\n",
    "    #print all of the best solutions found:\n",
    "    \n",
    "    #for i in range(0,15):\n",
    "     #   print (converter(hof.items[i]))\n",
    "        \n",
    "    \n",
    "    # print info for best solution found:\n",
    "       \n",
    "    print(\"-- Best Individual = \", hof.items[0])\n",
    "    print(\"-- Best Fitness--= \", hof.items[0].fitness.values[0])\n",
    "    \n",
    "    #print converted solutions here below\n",
    "    \n",
    "    best = hof.items[0]\n",
    "    \n",
    "    cum_co2 = converter_2(hof.items[0].fitness.values[0])\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness-- Cumulative CO2 = \", cum_co2)\n",
    "\n",
    "    # extract statistics:\n",
    "    maxFitnessValues, meanFitnessValues = logbook.select(\"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(maxFitnessValues, color='red')\n",
    "    plt.plot(meanFitnessValues, color='green')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Max / Average Fitness')\n",
    "    plt.title('Max and Average fitness over Generations')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#execute the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73c7c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2nd run with different random seed\n",
    "RANDOM_SEED = 2\n",
    "random.seed(RANDOM_SEED)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77314d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd run with different random seed\n",
    "RANDOM_SEED = 3\n",
    "random.seed(RANDOM_SEED)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th run with different random seed\n",
    "RANDOM_SEED = 4\n",
    "random.seed(RANDOM_SEED)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5th run with different random seed\n",
    "RANDOM_SEED = 5\n",
    "random.seed(RANDOM_SEED)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6th run with different random seed\n",
    "RANDOM_SEED = 6\n",
    "random.seed(RANDOM_SEED)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7th run with different random seed\n",
    "RANDOM_SEED = 7\n",
    "random.seed(RANDOM_SEED)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8th run with different random seed\n",
    "RANDOM_SEED = 8\n",
    "random.seed(RANDOM_SEED)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e8442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9th run with different random seed\n",
    "RANDOM_SEED = 9\n",
    "random.seed(RANDOM_SEED)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aaa233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10th run with different random seed\n",
    "RANDOM_SEED = 10\n",
    "random.seed(RANDOM_SEED)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af4d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
